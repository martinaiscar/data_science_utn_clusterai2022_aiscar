{"cells":[{"cell_type":"markdown","source":["## **Trabajo Práctico Final Grupal: Bank Subscription**\n","____\n","__Universidad Tecnológica Nacional, Buenos Aires__\\\n","__Ingeniería Industrial__\\\n","__Ciencia de Datos - Curso I5521__\n","____"],"metadata":{"id":"qiFP0urI4qiF"},"id":"qiFP0urI4qiF"},{"cell_type":"markdown","source":["- Nombre: Candela\n","- Apellido: Abatedaga\n","- Legajo: 1673117\n","\n","\n","\n","- Nombre: Martín\n","- Apellido: Aiscar\n","- Legajo: 1635487"],"metadata":{"id":"b9oGgwyt4qk0"},"id":"b9oGgwyt4qk0"},{"cell_type":"markdown","source":["**Entrega del trabajo: Viernes 25 de noviembre a las 23h59.**"],"metadata":{"id":"noMQ6rot4qnL"},"id":"noMQ6rot4qnL"},{"cell_type":"markdown","source":["##**Pipeline de Machine Learning**"],"metadata":{"id":"XnY3Z2o15G-5"},"id":"XnY3Z2o15G-5"},{"cell_type":"code","source":["## Dividimos el dataset en la variable dependiente \"y\", en este caso \"Subscription\", y las independientes \"x\".\n","y = np.array(bank_ds[[\"Subscription\"]])\n","x = bank_ds.drop(['Subscription'], axis=1)"],"metadata":{"id":"bAP2JAyj5IvA"},"id":"bAP2JAyj5IvA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Separamos el dataset en train y test. Corresponderán a test el 30% de las muestras.\n","xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=4)"],"metadata":{"id":"HWt6fjlC5Oz1"},"id":"HWt6fjlC5Oz1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Imprimimos las dimensiones del set de train y del de test.\n","print(f'Dimensiones del set de train: {xtrain.shape}')\n","print(f'Dimensiones del set de test: {xtest.shape}')"],"metadata":{"id":"4ni3AEDgGhVk"},"id":"4ni3AEDgGhVk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Diferenciamos las variables numéricas de las categóricas.\n","numeric_features = ['Age','Balance (euros)','Last Contact Day','Last Contact Month','Last Contact Duration','Campaign','Pdays','Previous']\n","\n","variables_cat= ['Job','Marital Status','Education','Credit','Housing Loan','Personal Loan']"],"metadata":{"id":"BOD5IyKd5QBT"},"id":"BOD5IyKd5QBT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Definimos la transformación a realizar para las variables numéricas. \n","## Las estandarizaremos utilizando StandardScaler.\n","numeric_transformer = Pipeline(\n","    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",")\n","\n","## Definimos la transformación a realizar para las variables categóricas.\n","## Generaremos dummies utilizado OneHotEncoder.\n","transformacion_cat = OneHotEncoder(handle_unknown=\"error\", sparse=False)"],"metadata":{"id":"Nyi9oCgh6Oju"},"id":"Nyi9oCgh6Oju","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Juntamos las transformaciones definidas previamente.\n","## Definimos el ColumnTransformer que será ejecutado al momento del fitting.\n","preprocesamiento = ColumnTransformer(\n","    transformers=[ \n","        (\"num\", numeric_transformer, numeric_features),\n","        (\"cat\", transformacion_cat, variables_cat),\n","    ]\n",")"],"metadata":{"id":"hyt_0hhtGwfZ"},"id":"hyt_0hhtGwfZ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Creamos un Pipeline para aplicar secuencialmente la lista de transformaciones definida previamente y un estimador final.\n","pipeline1 = Pipeline(\n","    steps=[(\"preprocesamiento\",preprocesamiento),(\"estimador\",SVC())]\n",")"],"metadata":{"id":"MeCjhmt86RED"},"id":"MeCjhmt86RED","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Definimos los hiperparámetros de los modelos a comparar.\n","parametros = [\n","    {\n","        \"estimador\": (LogisticRegression(),),\n","          \"estimador__C\": (0.001,1,10)\n","    },\n","    {\n","        \"estimador\": (SVC(),),\n","          \"estimador__kernel\":('linear', 'rbf'), \n","          \"estimador__C\":(0.1,1, 10), \n","          \"estimador__gamma\":(0.001, 0.01, 0.1,1, 10)\n","          \"estimador__probability\":(\"True\")          \n","    }\n","]"],"metadata":{"id":"R2zLJMln6SH-"},"id":"R2zLJMln6SH-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Creamos el Grid Search + Cross Validation.\n","grid_search1 = GridSearchCV(pipeline1, parametros,\n","                  refit = True,\n","                   cv = 5,\n","                   verbose=40)\n","grid_search1.fit(xtrain,ytrain.ravel())"],"metadata":{"id":"yr_wdrde6VNP"},"id":"yr_wdrde6VNP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Imprimimos cuál fue el mejor modelo junto con los mejores hiperparámetros.\n","print(\"Los mejores parámetros fueron %s con un score de %0.4f\" % (grid_search1.best_params_, grid_search1.best_score_))"],"metadata":{"id":"MdKA19wk6Yi8"},"id":"MdKA19wk6Yi8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Realizamos las predicciones sobre las muestras de test.\n","ypred1 = grid_search1.predict(xtest)\n","print(ypred1)"],"metadata":{"id":"ntT4IkLd6bnt"},"id":"ntT4IkLd6bnt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Computamos el accuracy.\n","test_acc = accuracy_score(ytest, ypred1)\n","print(\"El accuracy es \" + str(test_acc))"],"metadata":{"id":"spCST7iB6eWm"},"id":"spCST7iB6eWm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Computamos el área debajo de la curva ROC (AUC).\n","yproba1 = grid_search1.predict_proba(xtest)\n","fpr1, tpr1, thresholds = roc_curve(ytest.astype(int), yproba1[:,1], drop_intermediate = False, pos_label=2)\n","auc_value = auc(fpr1, tpr1)\n","print(\"El AUC es = \" + str(auc_value))"],"metadata":{"id":"eBKcqvoU6fdk"},"id":"eBKcqvoU6fdk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Calculamos e imprimimos la matriz de confusión.\n","cm = confusion_matrix(ytest, ypred1)\n","df_cm = pd.DataFrame(cm, index = ['No accede', 'Accede'], columns = ['No accede', \"Accede\"])\n","plt.figure(figsize = (6,4))\n","sns.heatmap(df_cm, annot=True,fmt='g')\n","plt.title('Matriz de Confusión')\n","plt.show()"],"metadata":{"id":"9Jz3oq3VHgs_"},"id":"9Jz3oq3VHgs_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Analisis de Componentes Pricipales**"],"metadata":{"id":"ZkSz7SH36kZH"},"id":"ZkSz7SH36kZH"},{"cell_type":"code","source":["## Definimos la proporción de variación que queremos explicar con los componentes a extraer.\n","n_comps = 0.8\n","\n","## Definimos el PCA.\n","pca = PCA(n_components = n_comps)\n","\n","## Definimos las transformaciones a realizar para las variables numéricas. \n","## Las estandarizaremos utilizando StandardScaler y luego aplicamos el método PCA.\n","numeric_transformer1 = Pipeline(\n","    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler()), (\"pca\", pca)]\n",")\n","\n","## Juntamos las transformaciones definidas previamente.\n","## Definimos el ColumnTransformer que será ejecutado al momento del fitting.\n","preprocesamiento1 = ColumnTransformer(\n","    transformers=[ \n","        (\"num\", numeric_transformer1, numeric_features),\n","        (\"cat\", transformacion_cat, variables_cat),\n","    ]\n",")\n","\n","## Definimos un nuevo pipeline que incluye el método PCA en el preprosemiento.\n","pipeline2=Pipeline(\n","    steps=[('preprocesamiento',preprocesamiento1),('estimador',SVC())]\n",")"],"metadata":{"id":"Vje9vYPd6lZN"},"id":"Vje9vYPd6lZN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Creamos el Grid Search + Cross Validation. \n","grid_search2 = GridSearchCV(pipeline2, parametros,\n","                  refit = True, ## Refit nos devuelve el modelo con los mejores parametros encontrados.\n","                   cv = 5, ## Indica la cantidad de folds.\n","                   verbose=40)\n","## Entrenamos.\n","grid_search2.fit(xtrain,ytrain.ravel())"],"metadata":{"id":"fpddmmU86oG1"},"id":"fpddmmU86oG1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Imprimimos cuál fue el mejor modelo y la combinación de hiper parámetros.\n","print(\"Los mejores parametros fueron %s con un score de %0.2f\" % (grid_search2.best_params_, grid_search2.best_score_))"],"metadata":{"id":"Gz1TRx236qyL"},"id":"Gz1TRx236qyL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Realizamos las predicciones sobre las muestras de test.\n","ypred2 = grid_search2.predict(xtest)\n","print(ypred2)"],"metadata":{"id":"56KWdusS6t-0"},"id":"56KWdusS6t-0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Computamos el accuracy.\n","test_acc = accuracy_score(ytest, ypred2)\n","print(\"El accuracy es \" + str(test_acc))"],"metadata":{"id":"KYuKcfGM6uGq"},"id":"KYuKcfGM6uGq","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Computamos el AUC ROC.\n","yproba2 = grid_search2.predict_proba(xtest)\n","fpr1, tpr1, thresholds = roc_curve(ytest.astype(int), yproba2[:,1], drop_intermediate = False, pos_label=2)\n","auc_value = auc(fpr1, tpr1)\n","print(\"El AUC es = \" + str(auc_value))"],"metadata":{"id":"fvXT1Hz_6wco"},"id":"fvXT1Hz_6wco","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Calculamos e imprimimos la matriz de confusión.\n","cm = confusion_matrix(ytest, ypred2)\n","df_cm = pd.DataFrame(cm, index = ['No accede', 'Accede'], columns = ['No accede', \"Accede\"])\n","plt.figure(figsize = (6,4))\n","sns.heatmap(df_cm, annot=True,fmt='g')\n","plt.title('Matriz de Confusión con PCA')\n","plt.show()"],"metadata":{"id":"c2Jmfd9mIBSn"},"id":"c2Jmfd9mIBSn","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}